---
- name: Create multiple VMs on OpenStack and auto-register to Rancher
  hosts: localhost
  connection: local
  gather_facts: no

  vars:
    # Rancher details
    rancher_url: "https://k8smanager.comtegra.pl"
    rancher_token: "token-p8htt:zmjjkpcfzb5gdrmq5djfsm4wrzszk9kb8fbqjpng9pvkmqm4fz762w"
    cluster_name: "{{ cluster_name }}"

    # Survey variables from AWX
    vm_count: "{{ vm_count | int }}"
    node_roles: "{{ node_roles }}"
    vm_net: "{{ vm_net }}"
    vm_flavor: "{{ vm_flavor }}"
    vm_name_prefix: "{{ vm_name_prefix }}"

    # Static VM details
    vm_image: "4e04bf46-b48f-4bd2-a15a-efdd3a666d43"
    vm_keypair: "k8s"

    # OpenStack authentication
    os_auth_url: "https://10.1.71.88:5000/v3"
    os_project_name: "admin"
    os_project_id: "e9f88faebaa44e3893fd26f9d41f9927"
    os_project_domain_id: "9021632abac74b018ee13e95c5e81bea"
    os_username: "admin"
    os_user_domain_name: "admin_domain"
    os_password: "Yaoki0kohghoQu9y"
    os_region_name: "RegionOne"
    os_interface: "public"
    os_identity_api_version: "3"

  tasks:

    ###########################################################################
    # VALIDATION
    ###########################################################################
    - name: Validate vm_count
      fail:
        msg: "vm_count must be positive"
      when: vm_count < 1

    - name: Convert comma-separated roles into list
      set_fact:
        parsed_roles: "{{ node_roles.split(',') }}"

    - name: Ensure role count matches VM count
      assert:
        that:
          - parsed_roles | length == vm_count
        fail_msg: "Number of roles does not match number of VMs!"

    - name: Generate VM numbers
      set_fact:
        vm_list: "{{ query('sequence','start=1 end=' + vm_count|string) }}"

    - name: Build VM â†’ role map
      set_fact:
        vm_role_map: "{{ vm_list | zip(parsed_roles) | list }}"

    - debug:
        msg: "VM {{ vm_name_prefix }}-{{ item.0 }} will be {{ item.1 }}"
      loop: "{{ vm_role_map }}"

    ###########################################################################
    # RANCHER API CALLS
    ###########################################################################

    - name: Get Rancher cluster info
      uri:
        url: "{{ rancher_url }}/v3/clusters?name={{ cluster_name }}"
        method: GET
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
      register: cluster_info

    - name: Fail if cluster not found
      fail:
        msg: "Cluster {{ cluster_name }} not found in Rancher!"
      when: cluster_info.json.data | length == 0

    - name: Set Rancher cluster ID
      set_fact:
        cluster_id: "{{ cluster_info.json.data[0].id }}"

    - name: Debug cluster ID
      debug:
        msg: "Cluster ID: {{ cluster_id }}"

    - name: Check for existing registration tokens
      uri:
        url: "{{ rancher_url }}/v3/clusterRegistrationTokens?clusterId={{ cluster_id }}"
        method: GET
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
      register: existing_tokens

    - name: Create Rancher registration token if needed
      uri:
        url: "{{ rancher_url }}/v3/clusterRegistrationTokens"
        method: POST
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
        body_format: json
        body:
          clusterId: "{{ cluster_id }}"
        status_code: 201,200
      register: token_create
      when: existing_tokens.json.data | length == 0
      ignore_errors: yes

    - name: Wait for nodeCommand to be ready
      uri:
        url: "{{ rancher_url }}/v3/clusterRegistrationTokens?clusterId={{ cluster_id }}"
        method: GET
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
      register: reg_info
      until: reg_info.json.data[0].nodeCommand is defined and reg_info.json.data[0].nodeCommand != ""
      retries: 20
      delay: 5

    - name: Store node command and token
      set_fact:
        node_command: "{{ reg_info.json.data[0].nodeCommand }}"
        registration_token: "{{ reg_info.json.data[0].token }}"

    - debug:
        msg: "Node command obtained successfully (token: {{ registration_token[:10] }}...)"

    ###########################################################################
    # CLOUD-INIT GENERATION WITH FIXES
    ###########################################################################

    - name: Create cloud-init files
      copy:
        dest: "/tmp/cloud-init-{{ vm_name_prefix }}-{{ item.0 }}.yml"
        content: |
          #cloud-config
          hostname: {{ vm_name_prefix }}-{{ item.0 }}
          fqdn: {{ vm_name_prefix }}-{{ item.0 }}.local
          manage_etc_hosts: true
          
          package_update: true
          package_upgrade: true
          packages:
            - curl
            - wget
            - docker.io
            - jq
            - iptables
            - conntrack
            - nfs-common
            - apparmor-utils
            - net-tools
            - iputils-ping

          write_files:
            - path: /etc/hosts
              content: |
                127.0.0.1 localhost {{ vm_name_prefix }}-{{ item.0 }} {{ vm_name_prefix }}-{{ item.0 }}.local
                10.1.77.17 k8smanager.comtegra.pl
              owner: root:root
              permissions: '0644'
            
            - path: /etc/docker/daemon.json
              content: |
                {
                  "exec-opts": ["native.cgroupdriver=systemd"],
                  "log-driver": "json-file",
                  "log-opts": {
                    "max-size": "100m"
                  },
                  "storage-driver": "overlay2"
                }
              owner: root:root
              permissions: '0644'

          runcmd:
            - echo "=== Starting Node Registration for {{ vm_name_prefix }}-{{ item.0 }} as {{ item.1 }} ==="
            
            # Set hostname
            - hostnamectl set-hostname {{ vm_name_prefix }}-{{ item.0 }}
            
            # Install Docker and configure
            - systemctl enable docker
            - systemctl start docker
            - usermod -aG docker ubuntu
            - sleep 10
            
            # Test connectivity
            - echo "Testing connectivity to Rancher..."
            - ping -c 3 10.1.77.17 || echo "Ping test failed, continuing anyway..."
            - curl -k -I {{ rancher_url }} || echo "Curl test failed, continuing anyway..."
            
            # Prepare registration command
            - |
              echo "=== Preparing registration command ==="
              
              # Get the base command
              BASE='{{ node_command }}'
              
              # Debug: show command
              echo "Base command: $BASE"
              
              # Make curl insecure
              BASE=$(echo "$BASE" | sed 's/curl /curl --insecure /g')
              
              # Build final command
              FINAL="$BASE"
              
              # Add roles based on VM type
              {% if 'etcd' in item.1 %}
              FINAL="$FINAL --etcd"
              echo "Added etcd role"
              {% endif %}
              
              {% if 'controlplane' in item.1 %}
              FINAL="$FINAL --controlplane"
              echo "Added controlplane role"
              {% endif %}
              
              {% if 'worker' in item.1 %}
              FINAL="$FINAL --worker"
              echo "Added worker role"
              {% endif %}
              
              # Add node name
              FINAL="$FINAL --node-name {{ vm_name_prefix }}-{{ item.0 }}"
              
              # Add address if needed (this helps with IP assignment)
              FINAL="$FINAL --address \$(hostname -I | awk '{print \$1}')"
              
              # Add internal address
              FINAL="$FINAL --internal-address \$(hostname -I | awk '{print \$1}')"
              
              # Debug output (mask token)
              MASKED_CMD=$(echo "$FINAL" | sed 's/token=[^ ]*/token=*****/g')
              echo "Final command: $MASKED_CMD"
              echo "Full command saved to /root/rancher-join.sh"
              
              # Save command to file
              echo "$FINAL" > /root/rancher-join.sh
              chmod +x /root/rancher-join.sh
              
            # Execute with retry logic
            - |
              echo "=== Attempting registration ==="
              
              cat > /root/register-with-retry.sh << 'EOF'
              #!/bin/bash
              MAX_RETRIES=20
              RETRY_DELAY=30
              
              for i in $(seq 1 $MAX_RETRIES); do
                echo "Registration attempt $i/$MAX_RETRIES"
                echo "Running: $(cat /root/rancher-join.sh | sed 's/token=[^ ]*/token=*****/g')"
                
                if bash /root/rancher-join.sh 2>&1 | tee /var/log/rancher-registration.log; then
                  echo "âœ… Registration successful!"
                  echo "Sleeping 30 seconds to let node settle..."
                  sleep 30
                  exit 0
                else
                  echo "âŒ Attempt $i failed, retrying in $RETRY_DELAY seconds..."
                  sleep $RETRY_DELAY
                fi
              done
              
              echo "âŒ Registration failed after $MAX_RETRIES attempts"
              echo "Check logs at /var/log/rancher-registration.log"
              exit 1
              EOF
              
              chmod +x /root/register-with-retry.sh
              /root/register-with-retry.sh
            
            # Post-registration checks
            - echo "=== Post-registration checks ==="
            - docker ps
            - echo "Node registration completed for {{ vm_name_prefix }}-{{ item.0 }}"
            
      loop: "{{ vm_role_map }}"

    ###########################################################################
    # CREATE VMS WITH CONFIGURATION FIXES
    ###########################################################################

    - name: Create VM on OpenStack
      openstack.cloud.server:
        state: present
        name: "{{ vm_name_prefix }}-{{ item.0 }}"
        image: "{{ vm_image }}"
        flavor: "{{ vm_flavor }}"
        key_name: "{{ vm_keypair }}"
        network: "{{ vm_net }}"
        userdata: "{{ lookup('file', '/tmp/cloud-init-' + vm_name_prefix + '-' + item.0|string + '.yml') }}"
        auto_ip: true
        wait: true
        timeout: 900
        config_drive: true  # Important for cloud-init
        security_groups: default
        validate_certs: false
      environment:
        OS_AUTH_URL: "{{ os_auth_url }}"
        OS_PROJECT_NAME: "{{ os_project_name }}"
        OS_PROJECT_ID: "{{ os_project_id }}"
        OS_PROJECT_DOMAIN_ID: "{{ os_project_domain_id }}"
        OS_USERNAME: "{{ os_username }}"
        OS_USER_DOMAIN_NAME: "{{ os_user_domain_name }}"
        OS_PASSWORD: "{{ os_password }}"
        OS_REGION_NAME: "{{ os_region_name }}"
        OS_INTERFACE: "{{ os_interface }}"
        OS_IDENTITY_API_VERSION: "{{ os_identity_api_version }}"
      register: new_vms
      loop: "{{ vm_role_map }}"
      ignore_errors: yes

    - name: Wait for VMs to get IP addresses
      pause:
        minutes: 2
      when: new_vms is defined

    ###########################################################################
    # DISPLAY VM INFORMATION
    ###########################################################################

    - name: Check VM creation results
      debug:
        msg: |
          VM {{ vm_name_prefix }}-{{ item.0 }} creation status:
          {% set idx = item.0|int - 1 %}
          {% if new_vms.results[idx].failed %}
            FAILED: {{ new_vms.results[idx].msg }}
          {% else %}
            SUCCESS
            VM ID: {{ new_vms.results[idx].server.id }}
            Status: {{ new_vms.results[idx].server.status }}
            {% if new_vms.results[idx].server.addresses %}
              Networks:
              {% for network, addresses in new_vms.results[idx].server.addresses.items() %}
                - {{ network }}:
                {% for addr in addresses %}
                  - {{ addr.addr }} ({{ addr.version }})
                {% endfor %}
              {% endfor %}
            {% else %}
              WARNING: No network addresses found!
            {% endif %}
          {% endif %}
      loop: "{{ vm_role_map }}"
      when: new_vms is defined

    ###########################################################################
    # WAIT FOR CLUSTER ACTIVATION WITH BETTER MONITORING
    ###########################################################################

    - name: Wait for cluster to show nodes (initial)
      uri:
        url: "{{ rancher_url }}/v3/clusters/{{ cluster_id }}"
        method: GET
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
      register: cluster_check
      until: cluster_check.json.appliedSpec.rancherKubernetesEngineConfig is defined
      retries: 30
      delay: 10
      ignore_errors: yes

    - name: Monitor node registration
      uri:
        url: "{{ rancher_url }}/v3/nodes?clusterId={{ cluster_id }}"
        method: GET
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
      register: node_status
      until: node_status.json.data | length >= vm_count
      retries: 40
      delay: 15
      ignore_errors: yes

    - debug:
        msg: |
          Node Registration Status:
          Total nodes in cluster: {{ node_status.json.data | length }}
          {% for node in node_status.json.data %}
          - {{ node.nodeName }}: {{ node.state }} ({{ node.conditions[0].type }}: {{ node.conditions[0].status }})
          {% endfor %}

    - name: Wait for cluster to become active
      uri:
        url: "{{ rancher_url }}/v3/clusters/{{ cluster_id }}"
        method: GET
        headers:
          Authorization: "Bearer {{ rancher_token }}"
        validate_certs: no
      register: cluster_state
      until:
        - cluster_state.json.state == "active"
        - (cluster_state.json.conditions | selectattr('type', 'equalto', 'Ready') | map(attribute='status') | first) == "True"
      retries: 60
      delay: 15

    - name: Show final cluster status
      debug:
        msg: |
          âœ… CLUSTER PROVISIONING COMPLETE!
          ==================================
          Cluster: {{ cluster_name }}
          State: {{ cluster_state.json.state }}
          Provider: {{ cluster_state.json.provider }}
          
          Kubernetes Info:
          - Version: {{ cluster_state.json.version.gitVersion | default('N/A') }}
          - API Endpoint: {{ cluster_state.json.apiEndpoint | default('N/A') }}
          
          Node Summary:
          {% for node in node_status.json.data %}
          - {{ node.nodeName }}: {{ node.roles | join(',') }} ({{ node.state }})
          {% endfor %}
          
          Health Status:
          {% for condition in cluster_state.json.conditions %}
          - {{ condition.type }}: {{ condition.status }} 
            Message: {{ condition.message | default('OK') }}
          {% endfor %}

    ###########################################################################
    # CLEANUP
    ###########################################################################

    - name: Remove cloud-init temp files
      file:
        path: "/tmp/cloud-init-{{ vm_name_prefix }}-{{ item.0 }}.yml"
        state: absent
      loop: "{{ vm_role_map }}"

    - name: Final success message
      debug:
        msg: |
          ðŸŽ‰ SUCCESS! Cluster deployment completed.
          ========================================
          Cluster: {{ cluster_name }}
          Status: ACTIVE âœ“
          Nodes Created: {{ vm_count }}
          Roles: {{ node_roles }}
          
          Next steps:
          1. Access Rancher UI: {{ rancher_url }}
          2. Check cluster: {{ rancher_url }}/dashboard/c/{{ cluster_id }}/explorer
          3. Monitor nodes: kubectl get nodes
          
          Troubleshooting:
          - Check VM logs: SSH to nodes and check /var/log/cloud-init-output.log
          - Check registration: /var/log/rancher-registration.log on each node
          - Verify network connectivity between nodes and Rancher server
